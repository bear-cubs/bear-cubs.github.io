export const bearCubsLeaderboard = [
  {
    "category": "LLM baselines",
    "model": "GPT-4o zero-shot",
    "accuracy": {
      "all": "2.7%",
      "text_only": "5.4%",
      "multimodal": "0.0%"
    },
    "answer_label": {
      "correct": "3",
      "wrong": "53",
      "uncertain": "55",
      "none": "0"
    },
    "average_time": {
      "correct": "---",
      "wrong": "---",
      "uncertain": "---"
    }
  },
  {
    "category": "LLM baselines",
    "model": "DeepSeek R1 zero-shot",
    "accuracy": {
      "all": "8.1%",
      "text_only": "10.7%",
      "multimodal": "5.5%"
    },
    "answer_label": {
      "correct": "9",
      "wrong": "82",
      "uncertain": "19",
      "none": "1"
    },
    "average_time": {
      "correct": "---",
      "wrong": "---",
      "uncertain": "---"
    }
  },
  {
    "category": "LLM baselines",
    "model": "GPT-4o + Google Search",
    "accuracy": {
      "all": "0.0%",
      "text_only": "0.0%",
      "multimodal": "0.0%"
    },
    "answer_label": {
      "correct": "0",
      "wrong": "4",
      "uncertain": "0",
      "none": "107"
    },
    "average_time": {
      "correct": "---",
      "wrong": "---",
      "uncertain": "---"
    }
  },
  {
    "category": "LLM baselines",
    "model": "DeepSeek R1 + Google Search",
    "accuracy": {
      "all": "1.8%",
      "text_only": "3.6%",
      "multimodal": "0.0%"
    },
    "answer_label": {
      "correct": "2",
      "wrong": "16",
      "uncertain": "0",
      "none": "93"
    },
    "average_time": {
      "correct": "---",
      "wrong": "---",
      "uncertain": "---"
    }
  },
  {
    "category": "LLM baselines",
    "model": "Perplexity sonar-pro",
    "accuracy": {
      "all": "5.4%",
      "text_only": "8.9%",
      "multimodal": "1.8%"
    },
    "answer_label": {
      "correct": "6",
      "wrong": "30",
      "uncertain": "58",
      "none": "17"
    },
    "average_time": {
      "correct": "---",
      "wrong": "---",
      "uncertain": "---"
    }
  },
  {
    "category": "Non-CU agents",
    "model": "Grok3 DeepSearch",
    "accuracy": {
      "all": "11.7%",
      "text_only": "21.4%",
      "multimodal": "1.8%"
    },
    "answer_label": {
      "correct": "13",
      "wrong": "95",
      "uncertain": "2",
      "none": "1"
    },
    "average_time": {
      "correct": "1:09",
      "wrong": "1:24",
      "uncertain": "2:05"
    }
  },
  {
    "category": "Non-CU agents",
    "model": "OpenAI Deep Research",
    "accuracy": {
      "all": "36.0%",
      "text_only": "60.7%",
      "multimodal": "10.9%"
    },
    "answer_label": {
      "correct": "40",
      "wrong": "69",
      "uncertain": "1",
      "none": "1"
    },
    "average_time": {
      "correct": "4:37",
      "wrong": "9:00",
      "uncertain": "3:58"
    }
  },
  {
    "category": "Non-CU agents",
    "model": "Google Deep Research",
    "accuracy": {
      "all": "23.4%",
      "text_only": "42.9%",
      "multimodal": "3.6%"
    },
    "answer_label": {
      "correct": "26",
      "wrong": "39",
      "uncertain": "46",
      "none": "0"
    },
    "average_time": {
      "correct": "4:21",
      "wrong": "4:00",
      "uncertain": "4:39"
    }
  },
  {
    "category": "CU agents",
    "model": "Convergence AI Proxy",
    "accuracy": {
      "all": "12.6%",
      "text_only": "16.1%",
      "multimodal": "9.1%"
    },
    "answer_label": {
      "correct": "14",
      "wrong": "44",
      "uncertain": "34",
      "none": "19"
    },
    "average_time": {
      "correct": "1:52",
      "wrong": "2:41",
      "uncertain": "5:24"
    }
  },
  {
    "category": "CU agents",
    "model": "Anthropic Computer Use",
    "accuracy": {
      "all": "14.4%",
      "text_only": "19.6%",
      "multimodal": "9.1%"
    },
    "answer_label": {
      "correct": "16",
      "wrong": "22",
      "uncertain": "73",
      "none": "0"
    },
    "average_time": {
      "correct": "2:24",
      "wrong": "2:35",
      "uncertain": "3:35"
    }
  },
  {
    "category": "CU agents",
    "model": "OpenAI Operator",
    "accuracy": {
      "all": "23.4%",
      "text_only": "33.9%",
      "multimodal": "12.7%"
    },
    "answer_label": {
      "correct": "26",
      "wrong": "43",
      "uncertain": "13",
      "none": "29"
    },
    "average_time": {
      "correct": "2:59",
      "wrong": "3:58",
      "uncertain": "8:06"
    }
  },
  {
    "category": "CU agents",
    "model": "ChatGPT Agent",
    "accuracy": {
      "all": "65.8%",
      "text_only": "76.8%",
      "multimodal": "54.5%"
    },
    "answer_label": {
      "correct": "73",
      "wrong": "30",
      "uncertain": "3",
      "none": "5"
    },
    "average_time": {
      "correct": "9:16",
      "wrong": "16:14",
      "uncertain": "25:55"
    }
  },
  {
    "category": "Human",
    "model": "Human",
    "accuracy": {
      "all": "84.7%",
      "text_only": "83.6%",
      "multimodal": "85.7%"
    },
    "answer_label": {
      "correct": "94",
      "wrong": "14",
      "uncertain": "---",
      "none": "3"
    },
    "average_time": {
      "correct": "4:24",
      "wrong": "5:44",
      "uncertain": "---"
    }
  }
];
